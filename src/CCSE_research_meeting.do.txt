TITLE: Distillation of CCSE Research Meeting
AUTHOR: Danny Caballero, Ellen Karoline Henriksen, Morten Hjorth-Jensen, Anders Malthe-Sorensen, and Sunniva Rose
DATE: today

========= Executive Summary =========

In our meeting on 07 Mar 2017, we discussed research efforts with which CCSE should engage including what the foci of the research could be and what specific kinds of projects CCSE might do within those foci. In addition, we discussed the profile of students we might recruit as well as the focus of the advertisement for a postdoctoral researcher.

======= Research Foci =======

Research projects serve at least two purposes:
  1. To document local issues around teaching and learning in computation and to understand what effects CCSE efforts might have on teaching and learning efforts at UiO.
  2. To develop new knowledge about issues of teaching and learning in computation.
Purpose 1 can be characterized as more evaluation-oriented (e.g., of CCSE efforts) while purpose 2 is characterized as more research-oriented (i.e., generating new knowledge).

======= What makes CCSE unique? =======

In defining research efforts, it became apparent that emphasizing what makes CCSE (and UiO more broadly) unique was important. That is, some research studies can (at present) only be conducted through CCSE at UiO because the structures are in place to support such efforts (i.e., computation already taught across most of the physics courses) or there are parties interested in supporting such efforts (i.e., biosciences and economics consider computation relevant for their studies). In thinking about what research to conduct, highlighting both of these unique aspects provides opportunities to perform studies that could not be conducted in places where such aspects are absent.

===== Purpose 1: Evaluation of CCSE efforts =====

Possible research studies for purpose 1 are listed below:

=== Surveying the state of things ===

Understanding the state in which computational instruction might exist is important. Developing a survey of students and faculty to be distributed across Norway would provide information on this state and allow us to track changes to that state and which changes may be attributed to CCSE activities and collaborations. Developing such a survey might follow the work bioCEED. In addition, we could leverage focus group interviews with students and faculty to form more detailed questions that can dig into details about the attitudes about and stances towards computational instruction. These focus group interviews might serve double duty: that is we might be able to report students' and faculty preliminary attitudes and stances surrounding computational instruction and the changes following their interaction with CCSE over time.

=== Documenting student learning and engagement in courses ===

Using existing and developed assessments of student learning and engagement we could provide quantifiable evidence of existing learning and engagement practices as well as changes to that learning and engagement over time. Some existing tools to gain evidence of student learning of analytical and conceptual ideas exist for upper-level courses (e.g., the "CCMI": "http://www.colorado.edu/physics/EducationIssues/ClassicalMechanics/download.html" and the "CUE": "http://www.colorado.edu/physics/EducationIssues/Electrostatics/download.html"). There are also some assessments of student attitudes about physics that exist (e.g., the "CLASS":"http://www.colorado.edu/sei/surveys/Faculty/CLASS-PHYS-faculty.html"). In addition, we could consider other aspects such as motivation (e.g., the "MSLQ":"http://stelar.edc.org/instruments/motivated-strategies-learning-questionnaire-mslq") and attitudes towards computing (e.g., the "CAS":"http://stelar.edc.org/instruments/computing-attitudes-survey-cas"). Developed assessments might work towards characterizing better the learning outcomes for courses in which computation is integrated as there are no existing external tools for such efforts.

In addition to external measures, documenting student success including their course work might be used to measure learning, growth, and the the changes to both over time. Consistent and robust in-class evaluation strategies could be employed (e.g., the development of a consistent rubric for judging the quality of students' programs).

=== Documenting student attitudes about coursework, pedagogy, etc. ===

Mid-semester and end of course surveys of students would offer documentation of students' attitudes towards courses and pedagogy. Some of these materials are already collected as part of UiO teaching evaluation strategies. One comprehensive tool for understanding student attitudes towards courses is the "Student Assessment of Learning Goals (SALG) survey": "http://www.salgsite.org". This particular survey offers sub-scales that can be used or not depending on the relevance to the instructor. Documenting changes to coursework and pedagogy will provide some evidence that changes to these might have resulted in changes to student attitudes.

=== Evaluating instructional approaches to teaching computation ===

Student learning is one aspect of evaluating instructional approaches (i.e., instruction is successful when students learn the material). But, also important is the approach itself as students' learning opportunities are tied to the instruction that is available to them. Approaches that engage the students in more active ways provide more opportunities for students to construct their understanding of the material, which research has shown to be important for students to develop a rich and robust understanding. There are several existing evaluations of instructional approaches that can be used including the "Reformed Teaching Observation Protocol (RTOP)":"http://physicsed.buffalostate.edu/AZTEC/RTOP/RTOP_full/index.htm" and the "Classroom Observation Protocol for Undergraduate STEM (COPUS)":"http://www.lifescied.org/content/12/4/618.short". These research-based tools have been used to provide faculty with formative feedback on their instruction, but they are not without flaws or controversy.

===== Purpose 2: Generating new knowledge around teaching and learning =====

=== Documenting conceptual, procedural, and epistemic resources and structures ===

There is a dearth of information on the resources (bits of reusable knowledge) that students hold with regard to computing in science (and in physics, in particular). These resources can be conceptual (e.g., knowledge about physical principles and concepts), procedural (e.g., knowledge about processes or actions one makes towards solving a problem), or epistemic (e.g., knowledge about the nature of knowledge they hold). These resources can be connected together in productive and unproductive ways -- unproductive resources or unproductive connections between them is a different way of thinking about misconceptions.

These ways of using resources can form larger epistemic structures such as epistemic games and epistemic frames. Epistemic games are a set of rules and strategies that characterize the ways in which students activate or organize resources when engaged in some task. The construct of epistemic games could help us describe and characterize the different ways that students engage in computational activities including how they negotiate computational modeling challenges. An epistemic frame is the network of activations and inhibitions of resources in response to a person's current activity. The construct of epistemic frames will help us understand how students frame computational activities including what resources students choose to bring to bear on them, and how interactions with within their learning environment with teachers and fellow students influence student reasoning and actions.


=== Studying the development of conceptual, procedural, and epistemic resources and structures ===

While documenting resources and larger epistemic structures is important to know what knowledge students hold and how they are making use of it, we have a unique opportunity to student how students develop that knowledge and change their use of it over time. As UiO as has integrated computation into a variety of courses and continues to do so, we are able to conduct studies that investigate how these resources and structures develop as students gain more computational experience.

We can conduct either cross-sectional or longitudinal studies (or both). The advantage of a cross-sectional study is that it can be conducted in a known finite time (i.e., a semester), which allows for the execution of a research project in a timely manner. The disadvantages of a cross-sectional study include that one can only document how these resources and strategies might have changed. The same students are not being studied at each point in the program, rather, a representative group is being studied. The advantage of a longitudinal study is that one can make a convincing case that changes to student's resources and uses of them can be attributed to specific experiences that a student had. The disadvantages include keeping students in the study over a long period of time as well as the time itself; it takes several years to conduct such a study, which might be longer than the lifetime of a graduate student researcher.

=== Studying the effect of computation on understanding science and science epistemology ===

How students think about science differently as a result of exposure to computation has been documented in some cases (e.g., early work by Sherin). There are a variety of studies that might be conducted here to explore understanding of science (e.g., how students think about differential equations differently), student performance (e.g., how successful students are at solving differential equations of different types), and epistemology (e.g., in what ways are different methods of solving differential equations useful).

The study designs for any of this work could be quasi-experimental and/or controlled studies. Controlled studies that are conducted in the classroom should employ an approach where inequities in learning opportunities do not persist (i.e., using a module of instruction that is swapped after the experiment is conducted). Controlled out of class studies do not have such ethical dilemmas and, thus, need not abide such concerns about equity.

=== Developing assessments of student understanding ===

There exist no external assessments of student understanding of computation in science at the levels our research will be conducted. As such, the development and testing of such assessments is important work for CCSE and could potentially be quite public work (i.e., with other institutions making use of these assessments). Moreover, there are a number of studies that could inform the development of these assessment instruments (e.g., what resources students develop and the variety of those resources) as well as a number of studies that could make use of such instruments (e.g., comparing learning environments, predicting student success). To be widely applicable, such assessments could make use of Rasch modeling in their development, which can be a computationally interesting problem unto itself.
